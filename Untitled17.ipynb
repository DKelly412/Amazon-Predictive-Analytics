{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b8bf73b-9d98-4134-a186-da557ed1b313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import the LinearRegression class from scikit-learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# Scikit-learn (machine learning)\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Import libraries and modules for data visualization\n",
    "from pandas.plotting import scatter_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Import scit-Learn module for the algorithm/model: Linear Regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import scit-Learn module for K-fold cross-validation - algorithm/model evaluation & validation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "815e0013-c5a8-42f1-8e9b-fe79cdb600de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The raw file has no header row, so we add column names manually\n",
    "col_names = [\n",
    "    \"Age\", \"Emp_type\", \"Fnlwgt\", \"Education\",\n",
    "    \"Education_num\", \"Martial\", \"Occupation\",\n",
    "    \"Relationship\", \"Race\", \"Sex\", \"Capital_gain\", \"Capital_loss\", \"weekly_hours\", \"Country\", \"Income\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04811617-b48b-4369-a422-a5fc9cf5225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/DKell/Downloads/adult_salary.csv', header=None, names=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0991de3-6e16-48ea-9f96-aaf60c316555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age           Emp_type  Fnlwgt   Education  Education_num  \\\n",
      "0  Age           Emp_type  Fnlwgt   Education  Education_num   \n",
      "1   39          State-gov   77516   Bachelors             13   \n",
      "2   50   Self-emp-not-inc   83311   Bachelors             13   \n",
      "3   38            Private  215646     HS-grad              9   \n",
      "4   53            Private  234721        11th              7   \n",
      "\n",
      "               Martial          Occupation    Relationship    Race    Sex  \\\n",
      "0              Marital          Occupation    Relationship    Race    Sex   \n",
      "1        Never-married        Adm-clerical   Not-in-family   White   Male   \n",
      "2   Married-civ-spouse     Exec-managerial         Husband   White   Male   \n",
      "3             Divorced   Handlers-cleaners   Not-in-family   White   Male   \n",
      "4   Married-civ-spouse   Handlers-cleaners         Husband   Black   Male   \n",
      "\n",
      "   Capital_gain  Capital_loss  weekly_hours         Country  Income  \n",
      "0  Capital_gain  Capital_loss  weekly_hours         Country  Income  \n",
      "1          2174             0            40   United-States   <=50K  \n",
      "2             0             0            13   United-States   <=50K  \n",
      "3             0             0            40   United-States   <=50K  \n",
      "4             0             0            40   United-States   <=50K  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48843 entries, 0 to 48842\n",
      "Data columns (total 15 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Age            48843 non-null  object\n",
      " 1   Emp_type       48843 non-null  object\n",
      " 2   Fnlwgt         48843 non-null  object\n",
      " 3   Education      48843 non-null  object\n",
      " 4   Education_num  48843 non-null  object\n",
      " 5   Martial        48843 non-null  object\n",
      " 6   Occupation     48843 non-null  object\n",
      " 7   Relationship   48843 non-null  object\n",
      " 8   Race           48843 non-null  object\n",
      " 9   Sex            48843 non-null  object\n",
      " 10  Capital_gain   48843 non-null  object\n",
      " 11  Capital_loss   48843 non-null  object\n",
      " 12  weekly_hours   48843 non-null  object\n",
      " 13  Country        48843 non-null  object\n",
      " 14  Income         48843 non-null  object\n",
      "dtypes: object(15)\n",
      "memory usage: 5.6+ MB\n",
      "None\n",
      "Age              0\n",
      "Emp_type         0\n",
      "Fnlwgt           0\n",
      "Education        0\n",
      "Education_num    0\n",
      "Martial          0\n",
      "Occupation       0\n",
      "Relationship     0\n",
      "Race             0\n",
      "Sex              0\n",
      "Capital_gain     0\n",
      "Capital_loss     0\n",
      "weekly_hours     0\n",
      "Country          0\n",
      "Income           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Quick look at the data\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b271f88e-266c-48eb-90a2-3959e0bcc50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Age        Fnlwgt  Education_num  Capital_gain  weekly_hours  \\\n",
      "count  48842.000000  4.884200e+04   48842.000000  48842.000000  48842.000000   \n",
      "mean      38.643585  1.896641e+05      10.078089   1079.067626     40.422382   \n",
      "std       13.710510  1.056040e+05       2.570973   7452.019058     12.391444   \n",
      "min       17.000000  1.228500e+04       1.000000      0.000000      1.000000   \n",
      "25%       28.000000  1.175505e+05       9.000000      0.000000     40.000000   \n",
      "50%       37.000000  1.781445e+05      10.000000      0.000000     40.000000   \n",
      "75%       48.000000  2.376420e+05      12.000000      0.000000     45.000000   \n",
      "max       90.000000  1.490400e+06      16.000000  99999.000000     99.000000   \n",
      "\n",
      "       Income  \n",
      "count     0.0  \n",
      "mean      NaN  \n",
      "std       NaN  \n",
      "min       NaN  \n",
      "25%       NaN  \n",
      "50%       NaN  \n",
      "75%       NaN  \n",
      "max       NaN  \n"
     ]
    }
   ],
   "source": [
    "#Check for abnormalmalities\n",
    "\n",
    "print(df.describe())\n",
    "\n",
    "# Replace '?' with proper missing values\n",
    "df = df.replace('?', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35d9397f-830f-41df-86fa-8a0fb52f3957",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age 0 values <= 0\n",
      "Education_num 0 values <= 0\n",
      "Capital_gain 0 values <= 0\n",
      "Capital_loss 0 values <= 0\n",
      "weekly_hours 0 values <= 0\n",
      "Income 0 values <= 0\n"
     ]
    }
   ],
   "source": [
    "# Check for zero or negative values in numeric columns\n",
    "numeric_cols = [\"Age\",\"Education_num\",\"Capital_gain\", \"Capital_loss\",\n",
    "                \"weekly_hours\",\"Income\"]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    # Convert column to numeric type first, with errors='coerce' to handle non-numeric values\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    print(col, (df[col] <= 0).sum(), \"values <= 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b49262a-485c-475c-9f8c-9f32c2fec938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['Age', 'Emp_type', 'Education', 'Education_num', 'Martial', 'Occupation', 'Relationship', 'Race', 'Sex', 'Capital_gain', 'Capital_loss', 'weekly_hours', 'Country', 'Income']\n",
      "Shape of X: (0, 13)\n",
      "Shape of y: (0,)\n",
      "Income unique labels: []\n"
     ]
    }
   ],
   "source": [
    "# First, check the actual column names in the dataframe\n",
    "print(\"Available columns:\", df.columns.tolist())\n",
    "\n",
    "# 2. Remove Fnlwgt if it exists (with correct spelling/capitalization)\n",
    "# Look for similar column names like 'fnlwgt' (lowercase) or other variations\n",
    "if 'fnlwgt' in df.columns:\n",
    "    df = df.drop(columns=['fnlwgt'])\n",
    "elif 'Fnlwgt' in df.columns:\n",
    "    df = df.drop(columns=['Fnlwgt'])\n",
    "# If you're sure the column should exist, you might need to check for typos\n",
    "# or ensure the data was loaded correctly\n",
    "\n",
    "# Drop rows where Income (the target) is missing, if any\n",
    "df = df.dropna(subset=['Income'])\n",
    "\n",
    "# Separate predictors (X) and target (y)\n",
    "X = df.drop(columns=['Income'])\n",
    "y = df['Income']\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "print(\"Income unique labels:\", y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3d237f9-c453-45d6-8bf7-830a494f14a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataset for submission\n",
    "df_clean.to_csv(\"adult_salary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "908ef66d-f478-487e-be04-31c7ef5c92c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (2824, 6) (2824,)\n",
      "Test shape: (1211, 6) (1211,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create some sample data - this is what was missing in your code\n",
    "# Replace this with your actual data loading code\n",
    "n_samples = 4035\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a sample dataframe with the features you defined\n",
    "data = {\n",
    "    'age': np.random.normal(40, 10, n_samples),\n",
    "    'income': np.random.normal(50000, 15000, n_samples),\n",
    "    'years_experience': np.random.normal(10, 5, n_samples),\n",
    "    'gender': np.random.choice(['M', 'F'], n_samples),\n",
    "    'education': np.random.choice(['HS', 'BS', 'MS', 'PhD'], n_samples),\n",
    "    'occupation': np.random.choice(['Engineer', 'Doctor', 'Teacher', 'Other'], n_samples)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a target variable - this was missing in your code\n",
    "# Replace with your actual target variable\n",
    "y = np.random.randint(0, 2, n_samples)  # Binary classification target\n",
    "X = df  # Your feature dataframe\n",
    "\n",
    "# Define your feature lists\n",
    "numeric_features = ['age', 'income', 'years_experience']\n",
    "categorical_features = ['gender', 'education', 'occupation']\n",
    "\n",
    "# Numeric preprocessing: impute missing with median\n",
    "numeric_transformer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Categorical preprocessing: impute most frequent, then one-hot encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine them\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Choose model: Logistic Regression for classification\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Full pipeline: preprocessing + model\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocess', preprocess),\n",
    "    ('model', log_reg)\n",
    "])\n",
    "\n",
    "# Train-test split (e.g. 70/30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=7, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4e5f24f-7a4b-43f8-bde0-604ec1d884ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train shape: (2703,)\n",
      "Y_train head:\n",
      "31514   NaN\n",
      "44052   NaN\n",
      "37042   NaN\n",
      "27384   NaN\n",
      "10286   NaN\n",
      "Name: Income, dtype: float64\n",
      "\n",
      "NaNs in Y_train:\n",
      "2703\n",
      "\n",
      "Number of non-NaN labels in Y_train: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Y_train shape:\", Y_train.shape)\n",
    "print(\"Y_train head:\")\n",
    "print(Y_train.head())\n",
    "\n",
    "print(\"\\nNaNs in Y_train:\")\n",
    "print(Y_train.isna().sum())  # if Series\n",
    "# If Y_train is a DataFrame, uncomment this:\n",
    "# print(Y_train.isna().sum(axis=0))\n",
    "\n",
    "print(\"\\nNumber of non-NaN labels in Y_train:\",\n",
    "      Y_train.shape[0] - Y_train.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca2ca698-71d6-4a74-b11e-9a2c25a30cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All values in Y_train are NaN\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression  # Added import for the model\n",
    "\n",
    "# Create a model instance (was missing in original code)\n",
    "model = LinearRegression()\n",
    "\n",
    "# Before fitting:\n",
    "# Check if Y_train exists and has values\n",
    "if 'Y_train' not in locals() or Y_train.empty:\n",
    "    print(\"Y_train is not defined or empty\")\n",
    "else:\n",
    "    # Create mask for non-NaN values\n",
    "    mask = ~pd.isna(Y_train)\n",
    "    \n",
    "    # Check if we have any valid data after masking\n",
    "    if mask.sum() > 0:\n",
    "        X_train_clean = X_train_numeric.loc[mask]\n",
    "        Y_train_clean = Y_train.loc[mask]\n",
    "        \n",
    "        # Verify we have data before fitting\n",
    "        if len(X_train_clean) > 0 and len(Y_train_clean) > 0:\n",
    "            model.fit(X_train_clean, Y_train_clean)\n",
    "            \n",
    "            print(model.intercept_)\n",
    "            print(model.coef_)\n",
    "        else:\n",
    "            print(\"No valid data after filtering\")\n",
    "    else:\n",
    "        print(\"All values in Y_train are NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05eb1f33-34d3-4a91-810c-0c49861dc6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.49463253509496286\n"
     ]
    }
   ],
   "source": [
    "# 5. Train the model on the training set\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Check simple accuracy on test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "411e4360-732e-4419-8152-384550e8ac40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns used by clf:\n",
      "['age' 'income' 'years_experience' 'gender' 'education' 'occupation']\n"
     ]
    }
   ],
   "source": [
    "# Quick check: which columns does this clf expect now?\n",
    "print(\"Columns used by clf:\")\n",
    "print(clf.named_steps['preprocess'].feature_names_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e5be7c1d-5425-4dea-969a-45a703cabaee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "columns are missing: {'education', 'years_experience', 'occupation', 'gender', 'income', 'age'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 34\u001b[0m\n\u001b[0;32m      1\u001b[0m new_records \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([\n\u001b[0;32m      2\u001b[0m    {\n\u001b[0;32m      3\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m35\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m    }\n\u001b[0;32m     32\u001b[0m ])\n\u001b[1;32m---> 34\u001b[0m new_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(new_records)\n\u001b[0;32m     35\u001b[0m new_proba \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict_proba(new_records)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (label, probs) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(new_pred, new_proba), start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:787\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m    785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 787\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform(Xt)\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1090\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m   1088\u001b[0m     diff \u001b[38;5;241m=\u001b[39m all_names \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(column_names)\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m diff:\n\u001b[1;32m-> 1090\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns are missing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# ndarray was used for fitting or transforming, thus we only\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# check that n_features_in_ is consistent\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     _check_n_features(\u001b[38;5;28mself\u001b[39m, X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: columns are missing: {'education', 'years_experience', 'occupation', 'gender', 'income', 'age'}"
     ]
    }
   ],
   "source": [
    " new_records = pd.DataFrame([\n",
    "    {\n",
    "        \"Age\": 35,\n",
    "        \"Emp_type\": \"Private\",\n",
    "        \"Education\": \"Bachelors\",\n",
    "        \"Education_num\": 13,\n",
    "        \"Marital\": \"Married-civ-spouse\",\n",
    "        \"Occupation\": \"Prof-specialty\",\n",
    "        \"Relationship\": \"Husband\",\n",
    "        \"Race\": \"White\",\n",
    "        \"Sex\": \"Male\",\n",
    "        \"Capital_gain\": 0,\n",
    "        \"Capital_loss\": 0,\n",
    "        \"weekly_hours\": 40,\n",
    "        \"Country\": \"United-States\"\n",
    "    },\n",
    "    {\n",
    "        \"Age\": 28,\n",
    "        \"Emp_type\": \"Private\",\n",
    "        \"Education\": \"HS-grad\",\n",
    "        \"Education_num\": 9,\n",
    "        \"Marital\": \"Never-married\",\n",
    "        \"Occupation\": \"Sales\",\n",
    "        \"Relationship\": \"Not-in-family\",\n",
    "        \"Race\": \"Black\",\n",
    "        \"Sex\": \"Female\",\n",
    "        \"Capital_gain\": 0,\n",
    "        \"Capital_loss\": 0,\n",
    "        \"weekly_hours\": 30,\n",
    "        \"Country\": \"United-States\"\n",
    "    }\n",
    "])\n",
    "\n",
    "new_pred = clf.predict(new_records)\n",
    "new_proba = clf.predict_proba(new_records)\n",
    "\n",
    "for i, (label, probs) in enumerate(zip(new_pred, new_proba), start=1):\n",
    "    print(f\"Record {i}: predicted Income = {label}, Probabilities = {probs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d02c945-2722-4b7e-9daf-4fd42cd12267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d27e012-08ba-4d04-b4b6-79082b88314e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
